<?xml version="1.0" encoding="ISO-8859-1"?><rss version="2.0"><channel><title>Essays - Scott Fortmann-Roe</title><link>http://scott.fortmann-roe.com/</link><description>A collection of essays on topics related to modeling and simulation.</description><language>en-us</language><copyright>Copyright (C) 2012 Scott Fortmann-Roe</copyright><item><title>Accurately Measuring Model Prediction Error</title><description>When assessing the quality of a model, being able to accurately measure its prediction error is of key importance. Often, however, techniques of measuring error are used that give grossly misleading results. This can lead to the phenomenon of over-fitting where a model may fit the training data very well, but will do a poor job of predicting results for new data not used in model training. Here is an overview of methods to accurately measure model prediction error. </description><link>http://scott.fortmann-roe.com/docs/MeasuringError.html</link><pubDate>Tue, 01 May 2012 00:00:00 -0700</pubDate><guid>http://scott.fortmann-roe.com/docs/MeasuringError.html</guid></item><item><title>Understanding the Bias-Variance Tradeoff</title><description>When we discuss prediction models, prediction errors can be decomposed into two main subcomponents we care about: error due to &quot;bias&quot; and error due to &quot;variance&quot;. There is a tradeoff between a model's ability to minimize bias and variance. Understanding these two types of error can help us diagnose model results and avoid the mistake of over- or under-fitting.</description><link>http://scott.fortmann-roe.com/docs/BiasVariance.html</link><pubDate>Fri, 01 Jun 2012 00:00:00 -0700</pubDate><guid>http://scott.fortmann-roe.com/docs/BiasVariance.html</guid></item></channel></rss>